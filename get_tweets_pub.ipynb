{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capturando tuits\n",
    "\n",
    "[Pablo Haya](http://pablohaya.com), Ultima versión: Mar-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n",
    "Este _notebook_ presenta como utiliza Python para obtener tuits a través de la API pública de Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de credenciales de acceso\n",
    "\n",
    "\n",
    "La captura de tuits se va realizar empleando la API pública que provee Twitter. Antes de empezar es preciso dar alta una aplicación en Twitter, y obtener las credenciasles de acceso que autoricen a la aplicación a descargarse menciones desde la API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pasos para dar de alta una aplicación son:\n",
    "\n",
    "* Crea una cuenta en [Twitter](https://twitter.com/) si no se dispusiera antes.\n",
    "* Ve a https://apps.twitter.com/ y conectate con tu usuario.\n",
    "* Haz click en _Create New App_.\n",
    "* Rellena el formulario, incluido la dirección de website, aunque no se disponga.\n",
    "* Acepta los términos y condiciones y crearla.\n",
    "* En la página siguiente, haz click en _Keys and Access Tokens_ y copia tu _API key_ y _API secret_.\n",
    "* Baja al final de la página, haz clic en _Create my access token_, y copia _Access token_ y _Access token secret_ en los huecos correspondientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración de la conexión a la API\n",
    "\n",
    "Una vez creada la aplicación pasamos al script propiamente dicho que primeramente requiere importar la bibliotecas necesarias. Para poder acceder a la API de Twitter se va a utilizar la biblioteca [tweepy](http://www.tweepy.org/). También son necesarias la biblioteca `json`, para manipular los tweets en este formato, y la biblioteca `time` como veremos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bibliotecas necesarias\n",
    "import json\n",
    "import time\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es preciso añadir las credenciales obtenidas al crear la aplicación sustituyéndolas en el siguiente extracto de código: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Credenciales de acceso a la API de Twitter\n",
    "access_token = \"INSERTA ACCESS TOKEN\"\n",
    "access_token_secret = \"INSERTA ACCESS TOKEN SECRET\"\n",
    "consumer_key = \"INSERTA CONSUMER KEY\"\n",
    "consumer_secret = \"INSERTA CONSUMER SECRET\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza la autenticación empleando las credeciales definidas anteriormente, y se inicializa la conexión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Autenticación\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer tuits de la propia cronología\n",
    "\n",
    "La opción más directa para obtener un conjunto de tuits es acceder a la cronología (_timeline_) del propio usuario que ha creado la aplicación. Por defecto, la función [home_timeline](http://docs.tweepy.org/en/v3.5.0/api.html#API.home_timeline) permite acceder a los últimos 20 tuits publicados en la cronología.\n",
    "\n",
    "La descarga no es inmediata de manera que es posible que tarde unos segundos en aparecer el resultado. Mientras que se estén capturando aparecerá un asterisco [*] en vez del identificador de la celda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api = tweepy.API(auth)\n",
    "\n",
    "# Función que transforma un objeto Status en json\n",
    "# No está documentado en la API\n",
    "def to_json(status):    \n",
    "    json_str = json.dumps(status._json)\n",
    "    return json_str\n",
    "\n",
    "# Función que escribe en un fichero un conjuto de tuits en formato json\n",
    "# Los tuits se añaden al fichero, y en caso de que no existe se crea\n",
    "def write_json(tweets, filename):\n",
    "    n = 0\n",
    "    try:\n",
    "        with open(filename, 'a') as outfile:\n",
    "            for tweet in tweets:\n",
    "                outfile.write(to_json(tweet))\n",
    "                n = n + 1\n",
    "    except (IOError, OSError, Failure) as e:\n",
    "        pass\n",
    "    finally:\n",
    "        return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "public_tweets = api.home_timeline()\n",
    "n = write_json(public_tweets, 'home_timeline.json')\n",
    "print(\"Guardados %d tuits\" % n)\n",
    "\n",
    "# Imprime únicamente el texto cada tuit a modo de comprobación\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceder a los tuits publicados por un  usuario\n",
    "\n",
    "Es igual de fácil acceder a los tuits publicados por usuario mediante la función [user_timeline](http://docs.tweepy.org/en/v3.5.0/api.html#API.user_timeline). En este caso nos vamos a descargar los tuits [KDnuggets (@kdnuggets)](https://twitter.com/kdnuggets). El nombre de usuario se añade como parámetro de la función sin la @. El [máximo número de tuits](# https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline.html) que se pueden descargar en cada llamada es de 3.200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nos descargamos los 150 últimos tuits del timeline\n",
    "public_tweets = api.user_timeline(\"KDnuggets\", count=150)\n",
    "n = write_json(public_tweets, 'KDnuggets_timeline.json')\n",
    "print(\"Guardados %d tuits\" % n)\n",
    "\n",
    "# Imprimos únicamente los 20 primeros tuits\n",
    "for tweet in public_tweets[1:20]:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Obteniendo cualquier tuit \n",
    "\n",
    "En general, podemos preguntar a la API para poder descargarnos cualquier tuit que contenga los términos queramos medianta la función [search](http://docs.tweepy.org/en/v3.5.0/api.html#API.search). Es importante tener en cuenta que la API pública tiene ciertas restricciones que impide descargarse todos los posibles tuits cuando el volumen es muy elevado.\n",
    "\n",
    "Para ello se define un consulta que se codifica como un array de términos que se interpreta como un OR. Así para descargar tuits que contenga el término \"hadoop\" o \"spark\", se codificaría como `[\"hadoop\", \"spark\"]` \n",
    "\n",
    "También se pueden incluir términos relacionados mediante un AND si se separan con un espacio, de manera que es preciso que estén ambos términos para que se capture el tuit. En este caso, sería necesario especificar mejor que tuits habría capturar relativos a _Spark_, ya que es un palabra que puede aparecer en otros contextos distintos de tecnologías de Big Data.\n",
    "\n",
    "Por ejemplo: si quisiéramos la siguiente consulta \"hadoop\" OR (\"apache\" AND \"spark\") se codificará como `[\"hadoop\", \"apache spark\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "public_tweets = api.search([\"hadoop\", \"apache spark\"])\n",
    "n = write_json(public_tweets, 'hadoop_or_apachespark.json')\n",
    "print(\"Guardados %d tuits\" % n)\n",
    "\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturando tuits en tiempo real\n",
    "\n",
    "A veces queremos capturar tuit de manera continuada. Para ello podemos definir una consulta que se descarge un flujo de tuits a medida que se van publicando en Twitter.\n",
    "\n",
    "Primeramente hay que declararse una clase de tipo `StreamListener` que gestione el flujo de tuits defiendo que se quiere hacer con cada uno, y cuando se termina la captura. \n",
    "\n",
    "La siguiente clase se encarga de guardar en fichero cada tuit recibido. Se inicializa con el nombre del fichero, y con el tiempo durante el cual se estarán capturando los tuits. Cada vez que se recibe un tuit se llama el método `on_data` que se encarga de comprobar si se ha excedido la duración de la captura, y de guardar el tuit en el fichero. El retorno de este función indica si se continua `True` o no `False` con la descarga de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FileStreamListener(StreamListener):\n",
    "    # requiere el nombre del fichero donde almacenar los tweets, \n",
    "    # y el tiempo límite en segundos con el que se configura la captura.\n",
    "    def __init__(self, filename, time_limit=30):\n",
    "        self.start_time = time.time()\n",
    "        self.limit = time_limit\n",
    "        self.save_file = open(filename, 'a')\n",
    "        self.n = 0 # numero de tuits procesados\n",
    "        super(FileStreamListener, self).__init__()\n",
    "\n",
    "    def on_data(self, tweet):\n",
    "        if (time.time() - self.start_time) < self.limit:\n",
    "            #print(tweet)\n",
    "            self.save_file.write(tweet)\n",
    "            self.save_file.write('\\n')\n",
    "            self.n = self.n + 1\n",
    "            return True\n",
    "        else:\n",
    "            self.save_file.close()\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A continuación se inicializa el flujo de tuits, y se configura con una consulta similar a la hicimos anteriormente para extraer tuits sobre \"Hadoop\" y \"Apache Spark\".\n",
    "\n",
    "Recordar que es preciso esperar hasta que venza el tiempo de la captura definido (1 min.) para disponer del archivo con los tweets. Mientras que se estén capturando aparecerá un asterisco [*] en vez del identificador de la celda.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Conexión a stream data\n",
    "listener = FileStreamListener(filename='twitter_stream_data.json', time_limit=60)\n",
    "stream = tweepy.Stream(auth, listener)\n",
    "# Captura del stream de Twitter actualizaciones de estado que contenga los términos\n",
    "stream.filter(track = ['hadoop', 'apache spark'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Guardados %d tuits\" % listener.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturando tendencias en Twitter\n",
    "\n",
    "Para terminar vamos a ver como se puede obtener aquellos términos que están siendo tendencia (_trending topics_) en un momento dado. Hay que tener en cuenta que las tendencias se definen por zonas geográficas de manera que es precio especificar primeramente la región de interés sobre la cual se quieren obtener las tendencias. Cada región se identifica con un _woeid_. Por ejemplo, los _trending topics_ mundiales son el _woeid_ 1.\n",
    "\n",
    "A continuación se consultan todas las posibles regiones que existen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locations = api.trends_available()\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos fijamos en una localización en concreto (por ejemplo, Madrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    if loc['name'] == 'Madrid':\n",
    "        mad_woeid = loc['woeid']\n",
    "        break\n",
    "mad_woeid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos todas las tendencias de Madrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mad_tt = api.trends_place(mad_woeid)\n",
    "mad_tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Recorremos las tendencias, y nos descargamos los últimos tuits para cada una de ellas a partir del atributo ´query´, y los guardamos en fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtenemos todas las queries\n",
    "queries_tt = list(map(lambda tweet: tweet['query'], mad_tt[0]['trends']))\n",
    "\n",
    "for q in queries_tt:\n",
    "    public_tweets = api.search(q)\n",
    "    write_json(public_tweets, 'trending_topics.json')\n",
    "\n",
    "# Imprimimos unicamente los tuits de la última query\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios\n",
    "\n",
    "1. Modificar la descarga de tuits en tiempo real de manera incluyan los términos \"logistic regression\" o \"deep learning\".\n",
    "* Crear un nuevo `StreamListener` que termine de capturar tuits cuando haya descargado un número predeterminado. \n",
    "* Recuperar todos los _trending topics_ de España, Madrid y Barcelona en un único archivo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
